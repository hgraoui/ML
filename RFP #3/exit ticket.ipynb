{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02d216bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (2512198576.py, line 93)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\hgraoui\\AppData\\Local\\Temp\\ipykernel_7120\\2512198576.py\"\u001b[1;36m, line \u001b[1;32m93\u001b[0m\n\u001b[1;33m    plt.imshow(q_table[start_pos[0], start_pos[1], :].reshape\u001b[0m\n\u001b[1;37m                                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Maze Representation (Grid)\n",
    "# 0 = Empty, 1 = Wall, -1 = Fire, 2 = Flag\n",
    "maze = [\n",
    "    [0, 0, 0, 1, 0, 0],\n",
    "    [0, 1, 0, 1, 0, 0],\n",
    "    [0, 1, 0, -1, 0, 0],\n",
    "    [0, 0, 0, 1, 0, 2],\n",
    "    [0, 1, 0, 0, 1, 0],\n",
    "    [0, 0, 0, 0, 0, 0]\n",
    "]\n",
    "\n",
    "# Dimensions of the maze\n",
    "rows, cols = len(maze), len(maze[0])\n",
    "\n",
    "# Actions: up, down, left, right\n",
    "actions = [(0, -1), (0, 1), (-1, 0), (1, 0)]  # (row, col) changes\n",
    "\n",
    "# Initialize Q-table\n",
    "q_table = np.zeros((rows, cols, len(actions)))  # (state, action)\n",
    "\n",
    "# Learning parameters\n",
    "alpha = 0.1   # Learning rate\n",
    "gamma = 0.9   # Discount factor\n",
    "epsilon = 0.2 # Exploration rate\n",
    "episodes = 1000\n",
    "\n",
    "# Reward system\n",
    "def get_reward(state):\n",
    "    row, col = state\n",
    "    if maze[row][col] == -1:  # Fire\n",
    "        return -100\n",
    "    elif maze[row][col] == 2:  # Flag\n",
    "        return 100\n",
    "    elif maze[row][col] == 1:  # Wall\n",
    "        return -1\n",
    "    else:  # Empty space\n",
    "        return 1\n",
    "\n",
    "# Check if the next state is valid (within maze bounds and not a wall)\n",
    "def is_valid_state(state):\n",
    "    row, col = state\n",
    "    if 0 <= row < rows and 0 <= col < cols and maze[row][col] != 1:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Q-learning Algorithm\n",
    "for episode in range(episodes):\n",
    "    # Start from a random empty cell\n",
    "    start_state = (random.randint(0, rows-1), random.randint(0, cols-1))\n",
    "    while maze[start_state[0]][start_state[1]] != 0:\n",
    "        start_state = (random.randint(0, rows-1), random.randint(0, cols-1))\n",
    "\n",
    "    state = start_state\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Exploration vs Exploitation\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action_idx = random.randint(0, len(actions)-1)  # Exploration\n",
    "        else:\n",
    "            action_idx = np.argmax(q_table[state[0], state[1]])  # Exploitation\n",
    "\n",
    "        # Perform the chosen action\n",
    "        action = actions[action_idx]\n",
    "        next_state = (state[0] + action[0], state[1] + action[1])\n",
    "        \n",
    "        # Check if next state is valid\n",
    "        if not is_valid_state(next_state):\n",
    "            next_state = state  # Stay in the same state if invalid\n",
    "        \n",
    "        # Get the reward for the new state\n",
    "        reward = get_reward(next_state)\n",
    "        \n",
    "        # Update the Q-value using the Q-learning formula\n",
    "        best_next_action = np.argmax(q_table[next_state[0], next_state[1]])\n",
    "        q_table[state[0], state[1], action_idx] += alpha * (reward + gamma * q_table[next_state[0], next_state[1], best_next_action] - q_table[state[0], state[1], action_idx])\n",
    "        \n",
    "        state = next_state  # Move to the next state\n",
    "        \n",
    "        # If the robot reaches the flag or fire, end the episode\n",
    "        if reward == 100 or reward == -100:\n",
    "            done = True\n",
    "\n",
    "# After training, let's visualize the final Q-table.\n",
    "# We'll show the Q-values for the actions at the starting position of the robot.\n",
    "\n",
    "# Displaying Q-values for the starting position\n",
    "start_pos = (2, 2)  # For example, start from (2,2)\n",
    "plt.imshow(q_table[start_pos[0], start_pos[1], :].reshape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47a7716",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
